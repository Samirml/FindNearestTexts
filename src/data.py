import torch
from sklearn.metrics import f1_score
import numpy as np
from datasets import load_dataset
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import DataCollatorForTokenClassification
from tqdm import tqdm

def load_and_preprocess_data(tokenizer_name: str = "bert-base-cased") -> tuple:
    """
     Loads and preprocesses the CoNLL-2003 dataset.

     Args:
         tokenizer_name (str): Name of the tokenizer to be used for text tokenization.

     Returns:
         tokenized_datasets: Tokenized dataset.
         tokenizer: Tokenizer object used for text processing.
         ner_feature: NER label feature structure.
         label_names: List of NER class names.
         data_collator: Data collator for token classification.
     """
    dataset = load_dataset("conll2003")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, clean_up_tokenization_spaces=True)

    tokenized_datasets = dataset.map(
        lambda batch: tokenize_and_align(batch, tokenizer),
        batched=True,
        remove_columns=dataset['train'].column_names,
    )

    ner_feature = dataset["train"].features["ner_tags"]
    label_names = ner_feature.feature.names
    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, padding = True, label_pad_token_id = -100)

    return tokenized_datasets, tokenizer, ner_feature, label_names, data_collator



def align_labels_with_tokens(labels: list, word_ids: list) -> np.ndarray:
    """
    Aligns NER labels with tokenized words. Handles cases where a word is split into multiple tokens.

    Args:
        labels (list): List of original NER labels for words.
        word_ids (list): List of word IDs generated by the tokenizer.

    Returns:
        new_labels (np.ndarray): Array of aligned labels with respect to tokenized words.
    """
    new_labels = np.full(len(word_ids), -100)  # -100 – специальное значение
    current_word = None

    for i, word_id in enumerate(word_ids[1:-1]):
        if word_id != current_word:
            current_word = word_id
            new_labels[i + 1] = labels[word_id]
        else:
            label = labels[word_id]
            if label % 2 == 1:
                label += 1
            new_labels[i + 1] = label

    return new_labels

def tokenize_and_align(batch: dict, tokenizer: AutoTokenizer) -> dict:
    """
      Tokenizes text and aligns NER labels for a batch of data.

      Args:
          batch (dict): A batch of sentences and corresponding NER labels.
          tokenizer (AutoTokenizer): Tokenizer object used for tokenization.

      Returns:
          dict: Tokenized input IDs and aligned NER labels.
      """
    tokenized = tokenizer(batch["tokens"], is_split_into_words=True)
    all_labels = batch["ner_tags"]
    aligned_labels = []
    for i, labels in enumerate(all_labels):
        aligned = align_labels_with_tokens(labels, tokenized.word_ids(i))
        aligned_labels.append(aligned)
    return {
        'input_ids': tokenized['input_ids'],
        'labels': aligned_labels
    }